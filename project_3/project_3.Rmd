---
title: "project_3"
author: "Eli (Ilya) Bolotin"
date: "11/14/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(keras, ggplot2)
```

## Dataset description

## Problem description/ Objective of the project

## Visually describe your textual data and findings

## Variables: identify relevant variables that are being usde in the analysis

## Methodology: steps detailing how the analysis was carried out

### Read in articles data and compute number of articles

```{r}
# read in articles
articles <- read.csv("nyt-comments/ArticlesApril2017.csv", header = T)
articles <- articles[,c("articleID","headline")]

# num unique articles
num_articles <- length(articles$articleID)
num_articles
```

### Read in comments data and select comment text and label

```{r}
# read in data
comments <- read.csv("nyt-comments/CommentsApril2017.csv", header = T)
comments <- comments[,c("commentBody","newDesk")]

# examine dataset size
dim_comments <- dim(comments)
total_comments <- dim_comments[1]
total_comments
```

### Get a count of classes (news desk) and preview comments and classes

```{r}
# preview comments
head(comments$commentBody)

# count unique classes
num_classes <- length(unique(comments$newDesk))
num_classes
```

For the month of April there were over 243,832 comments for `num_articles` articles

### Split comments into train and test sets

```{r}
set.seed(30)
num_train_samples <- round(total_comments * 0.80)
train_indices <- sample(1:total_comments, num_train_samples, replace = FALSE)
test_indices <- setdiff(1:num_train_samples, train_indices)
```

```{r}
# define x and y vars
x_train <- comments$commentBody[train_indices]
y_train <- comments$newDesk[train_indices]
x_test <- comments$commentBody[test_indices]
y_test <- comments$newDesk[test_indices]

# detach original data to clear memory
rm(comments)
```

### Tokenize text

Next, we need to create a tokenizer that will map words to numbers.

```{r}
# create a tokenizer that maps words to numbers for every comment
tokenizer <- text_tokenizer() %>% fit_text_tokenizer(x_train)
```

### Examine tokenized data

```{r}
# Get total number of unique words from tokenizer
length(tokenizer$word_index)

# View first/last word indices
head(tokenizer$word_index)
tail(tokenizer$word_index)
```

### Vectorize words

One-hot-coding every word for every comment (i.e. producing a binary word-vector for every word of every comment) is very computationally expensive. Instead, we can use an embedded layer in a neural network to represent the significance of word/sequences in the form of predetermined weights.

To do this, we need to vectorize every comment according to the tokenized word indices created above.

```{r}
# vectorize train and test data
train_vectors <- texts_to_sequences(tokenizer, x_train)
test_vectors <- texts_to_sequences(tokenizer, x_test)

# view embeddings
head(train_vectors)
```

### Pad tokenized sequences

Due to the fact that every sequence (comment) is of variable length, we need to standardize each sequence input before it can be fed to the neural network (which expects a fixed size input). This step involves:

* Determine the average length of comments
* Set a max sequence length, called the input_size
* For sequences less than max input size, pad these vectors with 0s.

```{r}
train_vector_lengths <- sapply(train_vectors, length)
hist(tv_len, main = "Train Vector Lengths Distribution")
quantile(train_vector_lengths, probs = c(0.05, .95))
```

As we can see above, the histogram contains a highly left-skewed distribution. We can see that length bottoms out near 300 words per comment (sequence). However, the 95% quantile is at a length of 228. So we can use this length as the input_size.

```{r}
# set input dimensions
input_size = 228

# Pad sequences
x_train <- pad_sequences(train_vectors, maxlen = input_size, padding = "post")
x_test <- pad_sequences(test_vectors, maxlen = input_size, padding = "post")

# View padded sequences
head(x_train)
```

### Create keras model

```{r}
model <- keras_model_sequential()
```

### Created embedding layer to represent vectorized words in compact form

Below, we arbitrarily chose to represent each vector in 10 dimensions

```{r}
model %>% 
  layer_embedding(input_dim = input_size, output_dim = 10) %>%
  layer_global_average_pooling_1d()
```

### View model weights

```{r}
model$get_weights()
```

### Create neural network layers

```{r}
model %>% 
  layer_dense(units = 200, activation = "relu") %>% 
  layer_dense(units = 100, activation = "relu") %>% 
  layer_dense(units = 50, activation = "relu") %>% 
  layer_dense(units = num_classes, activation = "softmax") 
```

### View model summary

```{r}
summary(model)
```


## Findings along with the discussion

## References (last page)