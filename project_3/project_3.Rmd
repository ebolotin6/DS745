---
title: "project_3"
author: "Eli (Ilya) Bolotin"
date: "11/14/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(keras, ggplot2, dplyr, tidytext)
```

## Dataset description

The dataset that I have chosen to use for this is analysis/project is New York Times comments data [1]. This open-source dataset contains NYT articles and (reader) comments from January to May 2017 and January to April 2018. As mentioned in the linked dataset description, the comments' data contains "over 2 million comments and 34 features", while the articles' data contains more than 9000 articles 16 features.

## Objective

The objective of this project is to analyze the comments of NYT readers with the goal of classifying comments into distinct article topics. Article topics are defined by editors of the NYT in column entitled "newDesk" that we will review below.

## Methodology: steps detailing how the analysis was carried out

The methodology used in this project to perform **text classification** is a form of **supervised learning**. Namely, a neural network (multiple layer perceptron) is trained on *comments data* to predict *article topic*.

## Text visualization and variable identification

Before viewing comments data, it makes sense to first review the articles for which readers write comments.

### Read in articles data and compute number of articles

```{r}
# read in articles
articles <- read.csv("nyt-comments/ArticlesApril2017.csv", header = T)
articles <- as.data.frame(articles[,c("articleID","headline","newDesk")])

# num unique articles
num_articles <- length(articles$articleID)

# num unique article topics
num_topics <- length(unique(articles$newDesk))
```

For the month of April 2017, there were `num_articles` published by the NYT (each with distinct headlines). **Examples** of headlines include:

```{r}
head(articles)
```

All `num_articles` above belong to `num_topics` distinct topics. These topics are:

```{r echo=TRUE, message=FALSE, warning=FALSE}
topic_tally <- articles %>% count(newDesk, sort=TRUE)
names(topic_tally) <- c("topic","count")

ggplot(topic_tally, aes(x = reorder(topic, -count), y = count))+
  geom_bar(stat="identity", width=0.7)+
  labs(title="NYT Article Topics - April 2017", x="Topic", y="Article count") + 
  scale_fill_brewer(palette="Greens") + 
  theme_bw() + 
  theme(panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.line = element_blank(),
        axis.text.x = element_text(hjust = 1, angle = 50),
        axis.ticks = element_blank())

```

We will be training a neural network to predict the topics above based on user comments.

### Read in comments data and select comment text and label

```{r}
# read in data
comments <- read.csv("nyt-comments/CommentsApril2017.csv", header = T)
comments <- as_tibble(comments[,c("commentBody","newDesk")])

# examine dataset size
dim_comments <- dim(comments)
total_comments <- dim_comments[1]
```

There are a total of `total_comments` for April 2017.

### Preview comments and compute total classes

Below is a small sample of user comments and their respective topics (classes.)

```{r echo=TRUE}
# preview comments
head(comments)

# count unique classes
num_classes <- length(unique(comments$newDesk))
```

As mentioned above, there are `num_classes` possible comment classes.

### Split comments into train and test sets

```{r}
set.seed(30)
num_train_samples <- round(total_comments * 0.80)
train_indices <- sample(1:total_comments, num_train_samples, replace = FALSE)
test_indices <- setdiff(1:num_train_samples, train_indices)
```

```{r}
# define x and y vars
x_train <- comments$commentBody[train_indices]
y_train <- comments$newDesk[train_indices]
x_test <- comments$commentBody[test_indices]
y_test <- comments$newDesk[test_indices]

# detach original data to clear memory
rm(comments)
```

### Analyze text

The comments/text need to be converted into machine readable form (numeric form) before we can work with them. Let's first analyze how many unique words we have in our dataset.

```{r}
comment_id <- tibble(idx = length(x_train), text = as.character(x_train))
tokenized_tibble <- comment_id %>% 
  unnest_tokens(word, text)
num_unique_words <- length(unique(tokenized_tibble$word))
```

There are `num_unique_words` unique words in our corpus.

### Tokenize text

Create a tokenizer that will map words to numbers for every comment, up to a maximum of unique words. 

```{r}
vocab_len = num_unique_words
# create a tokenizer that maps words to numbers for every comment
tokenizer <- text_tokenizer(num_words = vocab_len) %>% fit_text_tokenizer(x_train)
len_tokens <- length(tokenizer$word_index)
```

### Examine tokenized data

```{r}
# Get total number of unique words from tokenizer
# vocab_len <- length(tokenizer$word_index)
# vocab_len

# View first/last word indices
head(tokenizer$word_index)
tail(tokenizer$word_index)
```

### Create binary vectors of words

One-hot-coding every word for every comment (i.e. producing a binary word-vector for every word of every comment) is very computationally expensive. Instead, with neural networks we can use an embedded layer to calculate a fixed number of weights/coefficients that will represent word-sequences in more compact form.

To do this, we need to vectorize every comment according to the tokenized word indices created above.

```{r}
# vectorize train and test data
train_vectors <- texts_to_sequences(tokenizer, x_train)
test_vectors <- texts_to_sequences(tokenizer, x_test)

# view embeddings
head(train_vectors, 1)
```

### Pad tokenized sequences

Due to the fact that every word-sequence (comment) is of variable length, each sequence should be standardized to a fixed length before it can be fed to the neural network (which expects every input to have a equal/fixed size).

**First**: Analyze the length of comments

```{r}
train_vector_lengths <- sapply(train_vectors, length)
hist(train_vector_lengths, main = "Train Vector Lengths Distribution")
quantiles = quantile(train_vector_lengths, probs = c(0.05, .95))
```

As we can see above, the histogram contains a highly left-skewed distribution. We can see that length bottoms out near 300 words per comment (sequence). However, the 95% quantile is equal to `quantiles[2]` words per comment. So we can use this length as the input_size.

**Next**: Set a max sequence length to `quantiles[2]` words and pad vectors less than this with 0.

```{r}
# set input dimensions
vector_len = quantiles[2]

# Pad sequences
x_train <- pad_sequences(train_vectors, maxlen = vector_len, padding = "post")
x_test <- pad_sequences(test_vectors, maxlen = vector_len, padding = "post")

# View padded sequences (1 sequence shown)
head(x_train, 1)
```

### Create binary matrix of target variables for train/test sets

```{r}
y_train <- as.factor(y_train)
y_train_numeric <- as.numeric(y_train)-1
y_train_bin <- to_categorical(y_train_numeric, num_classes)

y_test <- as.factor(y_test)
y_test_numeric <- as.numeric(y_test)-1
y_test_bin <- to_categorical(y_test_numeric, num_classes)
```

### Create Keras model

Below is the neural network architecture. We are using a sequential model with 4 fully connected layers, 1 pooling layer, and 2 dropout layers. Notice the use of the embedding layer to represent vectorized words in compact form (input dim = `vocab_len` and output = 100).

```{r message=FALSE, warning=FALSE}
# create model
model <- keras_model_sequential()

# define layers
model %>%
  layer_embedding(input_dim = len_tokens-1, output_dim = 100) %>%
  layer_global_average_pooling_1d() %>%  
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(rate = 0.10) %>% 
  layer_dense(units = 40, activation = "relu") %>% 
  layer_dropout(rate = 0.10) %>% 
  layer_dense(units = num_classes, activation = "softmax")

# view summary
summary(model)

# view weights
# model$get_weights()
```

### Define gradient descent optimizer, loss function, and metrics

```{r}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = 'accuracy'
)
```

### Train model 

```{r}
# create checkpoints folder to save model while training
checkpoint_dir <- "checkpoints"
dir.create(checkpoint_dir, showWarnings = FALSE)
filepath <- file.path(checkpoint_dir, "weights.{epoch:02d}-{val_loss:.2f}.hdf5")

# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
  filepath = filepath,
  save_weights_only = TRUE,
  save_best_only = TRUE,
  verbose = 0
)

early_stopping <- callback_early_stopping(patience = 2)

# train model
history <- model %>% fit(
  x_train, y_train_bin, 
  epochs = 1, 
  batch_size = 128, 
  validation_split = 0.05,
  verbose = 1,
  callbacks = list(cp_callback, early_stopping)  # pass callback to training
)

# model %>% save_model_hdf5("nyt_txt_class.h5")
```


### Plot model training

```{r}
plot(history)
```


### Evaluate model performance

```{r}
model %>% evaluate(x_test, y_test_bin, verbose = 0)
```


### Generate predictions on new data

```{r}
model %>% predict_classes(x_test)
```

## Findings along with the discussion

## References

[1] https://www.kaggle.com/aashita/nyt-comments