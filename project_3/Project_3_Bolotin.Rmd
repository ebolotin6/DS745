---
title: "DS 745: Project 3"
description: "DS 745"
author: "Eli (Ilya) Bolotin"
date: "11/14/2019"
output: 
  word_document:
    toc: true
    df_print: default
header-includes:
  - \usepackage{fvextra}
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(keras, ggplot2, dplyr, tidytext)
```

# Dataset description

The dataset I have chosen to use for this project is comments and articles data from the New York Times [1]. This open-source dataset contains NYT reader's comments for articles from January to May 2017 and January to April 2018. The comments data contains "over 2 million comments and 34 features", while the articles data contains more than 9000 articles and 16 features.

Due to the large size of the data, I have chosen 1 month's worth of comments data (April 2018) to analyze. As seen below, this month contains well over 240,000 comments, which is a sufficient volume for the scope of this project.

# Objective

The objective of this project is perform text classification to predict article topic from comments text. We will perform supervised learning on **comments text** and **section heading** to predict **article topic**. Article topics are defined by editors of the NYT in column entitled "newDesk".

# Methodology

The methodology used in this project to perform **text classification** is a form of **supervised learning** known as **deep learning**. Namely, we will train a neural network (multiple layer perceptron) to make predictions.

# Text visualization and analysis

Before viewing comments data, it makes sense to first review the articles for which readers write comments.

## Read in articles data and compute number of articles

```{r}
# read in articles
articles <- read.csv("nyt-comments/ArticlesApril2018.csv", header = T)
articles <- as_tibble(articles[,c("headline","newDesk", "sectionName")])

# num unique articles
num_articles <- length(articles$headline)

# num unique article topics
num_topics <- length(unique(articles$newDesk))
```

For the month of April 2018, there were `r num_articles` articles published by the NYT (each with distinct headlines). **Examples** of headlines include:

```{r echo=FALSE}
head(articles[articles$headline != "Unknown",])
```

All `r num_articles` articles above belong to `r num_topics` distinct topics. Below the top-25 most frequent article topics are plotted:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# create aggregate table
topic_tally <- articles %>% count(newDesk, sort=TRUE)
names(topic_tally) <- c("topic","count")

# plot topic frequency
ggplot(topic_tally[topic_tally$count>10,], aes(x = reorder(topic, -count), y = count))+
  geom_bar(stat="identity", width=0.7)+
  labs(title="NYT Article Topics - April 2018", x="Topic", y="Article count") + 
  scale_fill_brewer(palette="Greens") + 
  theme_bw() + 
  theme(panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.line = element_blank(),
        axis.text.x = element_text(hjust = .8, angle = 50, size = 10),
        axis.ticks = element_blank())

```

We will be training a neural network to predict the topics above based on **user comments** and **section heading/name**.

## Read in comments data

```{r}
# read in data
comments <- read.csv("nyt-comments/CommentsApril2018.csv", header = T)
comments <- as_tibble(comments[,c("commentBody","newDesk", "sectionName")])

# examine dataset size
dim_comments <- dim(comments)
total_comments <- dim_comments[1]
```

There are a total of `r total_comments` comments for April 2018.

## Preview comments and compute total classes

Below is a small sample of user comments:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# preview comments
head(comments)

# count unique classes
num_classes <- length(unique(comments$newDesk))
```

As mentioned above, there are `r num_classes` possible comment classes. 

Next, we will remove any comments with an "Unknown" section name, as well as any comments with NA, and drop unused levels.

```{r}
comments <- comments[comments$sectionName != 'Unknown',]
comments$commentBody <- as.character(comments$commentBody)
comments <- na.omit(comments)
comments <- droplevels.data.frame(comments)
num_rows <- nrow(comments)
```

We now have `r num_rows` comments to train on.

## Split comments into train and test sets

```{r}
set.seed(39)

# shuffle data to ensure train and test subsets contain approximately normally distributed classes
new_indices <- sample(num_rows, replace = FALSE)
comments <- comments[new_indices,]

# sample train and test indices
num_train_samples <- round(num_rows * 0.90)
train_indices <- sample(1:num_rows, num_train_samples, replace = FALSE)
test_indices <- setdiff(1:num_train_samples, train_indices)
```

```{r}
# create train/test datasets
x_train <- comments[train_indices, c("commentBody","sectionName")]
y_train <- comments$newDesk[train_indices]
x_test <- comments[test_indices, c("commentBody","sectionName")]
y_test <- comments$newDesk[test_indices]
```

## Analyze text

The comments/text need to be converted into machine readable form (numeric form) before we can work with them. Let's first analyze how many unique words we have in our dataset. Note that this is just a rough estimate, because we have not (yet) filtered symbols and other non-needed characters from the comments.

```{r}
comment_id <- tibble(idx = length(x_train), text = as.character(x_train))
tokenized_tibble <- comment_id %>% 
  unnest_tokens(word, text)
num_unique_words <- length(unique(tokenized_tibble$word))
```

There are approximately `r num_unique_words` unique words in our corpus.

## Tokenize text

Create a tokenizer that will map words to numbers for every comment (up to a maximum of unique words). 

```{r}
# create a tokenizer that maps words to numbers for every comment
vocab_len = num_unique_words
tokenizer <- text_tokenizer(num_words = vocab_len) %>% fit_text_tokenizer(x_train$commentBody)
```

## Examine tokenized data

```{r}
# Get actual number of unique words from tokenizer
vocab_len <- length(tokenizer$word_index)

# View first/last word indices
head(tokenizer$word_index)
tail(tokenizer$word_index)
```

## Create binary vectors of words

One-hot-coding every word for every comment (i.e. producing a binary word-vector for every word of every comment) is very computationally expensive. Instead, with neural networks we can use an embedded layer to represent word-sequences in more compact form (by using a fixed number of coefficients).

To do this, we need to vectorize every comment according to the tokenized word indices created above.

```{r}
# vectorize train and test data
train_vectors <- texts_to_sequences(tokenizer, x_train$commentBody)
test_vectors <- texts_to_sequences(tokenizer, x_test$commentBody)

# view sample sequence for first comment
head(train_vectors, 1)
```

## Pad tokenized sequences

Due to the fact that every word-sequence is of variable length, each sequence should be standardized to a fixed length before it can be fed to the neural network (which expects every input to have a common size).

**First**: Analyze the length of comments

```{r}
train_vector_lengths <- sapply(train_vectors, length)
hist(train_vector_lengths, main = "Train Vector Lengths Distribution")
quantiles = quantile(train_vector_lengths, probs = c(0.05, .95))
```

The histogram shows a highly right-skewed distribution. We can see that length bottoms out near 300 words per comment (sequence). However, the 95% quantile is equal to `r quantiles[2]` words per comment. So we can use this length as the input_size.

**Next**: Set a max sequence length to `r quantiles[2]` words and pad vectors less than this with zeros.

```{r}
# set input dimensions
vector_len = quantiles[2]

# Pad sequences
x_train <- pad_sequences(train_vectors, maxlen = vector_len, padding = "post")
x_test <- pad_sequences(test_vectors, maxlen = vector_len, padding = "post")

# VIew padded sequence (1 sequence shown)
head(x_train, 1)
```

Above is an example of a padded sequence.

## Create binary matrix of target variables for train/test sets

```{r}
y_train <- as.factor(y_train)
y_train_numeric <- as.numeric(y_train)-1
y_train_bin <- to_categorical(y_train_numeric, num_classes)

y_test <- as.factor(y_test)
y_test_numeric <- as.numeric(y_test)-1
y_test_bin <- to_categorical(y_test_numeric, num_classes)
```

## Create Keras model

Below is the neural network architecture. We are using a sequential model with 4 fully connected layers, 1 pooling layer, and 2 dropout layers. Notice the use of the embedding layer to represent vectorized words in compact form (input dim = `r vocab_len` and output = 300).

```{r warning=FALSE}
# create model
model <- keras_model_sequential()

# define layers
model %>%
  layer_embedding(input_dim = vocab_len, output_dim = 300) %>%
  layer_global_average_pooling_1d() %>%  
  layer_dense(units = 150, activation = "relu") %>%
  layer_dropout(rate = 0.15) %>% 
  layer_dense(units = 75, activation = "relu") %>% 
  layer_dropout(rate = 0.15) %>%
  layer_dense(units = num_classes, activation = "softmax")

# view summary
summary(model)
```


## Compile model

Define gradient descent optimizer, loss function, and metrics

```{r}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = 'accuracy'
)
```

## Define save/load directory

```{r}
# create checkpoints folder to save model while training
checkpoint_dir <- "checkpoints"
dir.create(checkpoint_dir, showWarnings = FALSE)
filepath <- file.path(checkpoint_dir, "best_model.hdf5")
```

## Train model

```{r eval=FALSE, warning=FALSE, include=TRUE}
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
  filepath = filepath,
  save_weights_only = TRUE,
  save_best_only = TRUE,
  verbose = 0
)

early_stopping <- callback_early_stopping(patience = 3)

# train model
history <- model %>% fit(
  x_train, y_train_bin, 
  epochs = 20, 
  batch_size = 1280, 
  validation_split = 0.20,
  verbose = 1,
  callbacks = list(cp_callback, early_stopping)  # pass callback to training
)
```

## Plot model training

```{r eval=FALSE, include=TRUE}
plot(history)
```

## Load best model for plotting

I included this step for those that wish to load a previously trained model in order to evaluate on a test set.

```{r}
load_model_weights_hdf5(model, filepath, by_name = FALSE,
  skip_mismatch = FALSE, reshape = FALSE)
```

## Evaluate model performance

```{r}
model %>% evaluate(x_test, y_test_bin, verbose = 0)
```

# Findings along with the discussion

As we see from the evaluation of the model on the test data, the model produces **~63%** classification accuracy on the test set (`r nrow(x_test)` observations and `r num_classes` classes). This is not a good result.

The training plot is highly indicative of what is happening here. From the training plot we see that model begins to overfit the training set at the 5th epoch. By the 10th epoch, validation accuracy has peaked at ~63%, while training accuracy peaks at 81% (13th epoch).

Why does the model have poor prediction accuracy on the test set? This is due to the generic nature of reader comments for NYT articles (and the topics created for them). Many comments are simply not characteristic of any singular topic. Take for example this subset of comments with a *newsDesk* title of 'Sports' and *sectionName* of 'Pro Football':

```{r}
slice(comments[comments$newDesk == "Sports" & comments$sectionName == "Pro Football",], 50:60)
```

The conversation in these comments is about harassment, lawsuits, and discrimination against cheerleaders. There is no actual conversation about sports, athletes, or NFL games. The more accurate classification topic here is politics or perhaps culture. Such is also the case with reader comments from NYT April 2017 regarding Colin Kaepernick. When Kaepernick created a political movement by taking a knee during the national anthem of a football game, all of the comments in response to the subsequent NYT article had to do with the political repercussions of his actions (as well as the reactions of Donald Trump) – thus diluting any actual sports talk from comments labeled as "sports". This is naturally confusing to neural nets.

Specifically in this project, the model (which assigns weights to words) has a difficult time correctly classifying statements in the test set that consist of the same words (but in different context) found in the training set. In the training set the model updates its parameters to minimize the loss function, thus eventually achieving a perfect recognition of which specific comments (meaning – a specific sequence of words) belong to distinct topics. But when these same weights are applied to the test set, which consists of largely the same words but in different context, the model cannot achieve similar accuracy – because inherently generic words in different order can belong to many topics. As such, the model cannot distinguish the correct classes with high enough probability. This is known as overfitting.

In conclusion, given the small amount of data used in this project and the short duration of training, the above neural network cannot yet classify polysemantic comments beyond ~61% accuracy.

# References

^[1]^ NYT Dataset, https://www.kaggle.com/aashita/nyt-comments

^[2]^ Keras for R, https://blog.rstudio.com/2017/09/05/keras-for-r/

^[3]^ Intro to text classification, https://www.onceupondata.com/2019/01/21/keras-text-part1/ 

^[4]^ Preparing data for NLP, https://shirinsplayground.netlify.com/2019/01/text_classification_keras_data_prep/

^[5]^ Text classification in R, https://www.jla-data.net/eng/vocabulary-based-text-classification/
